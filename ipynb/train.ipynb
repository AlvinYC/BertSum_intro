{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "    Main training workflow\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "import distributed\n",
    "from models import data_loader, model_builder\n",
    "from models.data_loader import load_dataset\n",
    "from models.model_builder import Summarizer\n",
    "from models.trainer import build_trainer\n",
    "from others.logging import logger, init_logger\n",
    "\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## command from BertSum github\n",
    "<python>\n",
    "python train.py \n",
    "    -mode train \n",
    "    -encoder classifier \n",
    "    -dropout 0.1 \n",
    "    -bert_data_path ../bert_data/cnndm \n",
    "    -model_path ../models/bert_classifier \n",
    "    -lr 2e-3 \n",
    "    -visible_gpus 0,1,2  \n",
    "    -gpu_ranks 0,1,2 \n",
    "    -world_size 3 \n",
    "    -report_every 50 \n",
    "    -save_checkpoint_steps 1000 \n",
    "    -batch_size 3000 \n",
    "    -decay_method noam \n",
    "    -train_steps 50000 \n",
    "    -accum_count 2 \n",
    "    -log_file ../logs/bert_classifier \n",
    "    -use_interval true \n",
    "    -warmup_steps 10000\n",
    "   </python>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device_id):\n",
    "    init_logger(args.log_file)\n",
    "\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "    logger.info('Device ID %d' % device_id)\n",
    "    logger.info('Device %s' % device)\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    if device_id >= 0:\n",
    "        torch.cuda.set_device(device_id)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    def train_iter_fct():\n",
    "        return data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n",
    "                                                 shuffle=True, is_test=False)\n",
    "\n",
    "    model = Summarizer(args, device, load_pretrained_bert=True)\n",
    "    if args.train_from != '':\n",
    "        logger.info('Loading checkpoint from %s' % args.train_from)\n",
    "        checkpoint = torch.load(args.train_from,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "        opt = vars(checkpoint['opt'])\n",
    "        for k in opt.keys():\n",
    "            if (k in model_flags):\n",
    "                setattr(args, k, opt[k])\n",
    "        model.load_cp(checkpoint)\n",
    "        optim = model_builder.build_optim(args, model, checkpoint)\n",
    "    else:\n",
    "        optim = model_builder.build_optim(args, model, None)\n",
    "\n",
    "    logger.info(model)\n",
    "    trainer = build_trainer(args, device_id, model, optim)\n",
    "    trainer.train(train_iter_fct, args.train_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
